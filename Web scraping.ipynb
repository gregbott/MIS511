{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping the Web\n",
    "Sometimes the data you require is not available in a structured, downloadable format. Often the most current data is available only on a web site. This notebook demonstrates how to \"scrape\" data from web sites using several different methods.\n",
    "\n",
    "> If you understand how to web scrape, any available data on the web is a database for you!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Should I use RegeX (regular expressions) to parse web data?**\n",
    "> Although extracting patterned data from a text file is directly in the wheelhouse of RegEx, I recommend *against* using it to parse HTML. Crafting an expression that returns all desired strings while excluding all undesired strings is likely to fail. HTML pages vary widely and will return or exclude data in ways you cannot anticipate. Instead, use a library such as Beautiful Soup, to parse HTML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing Source HTML\n",
    "To determine the best method for extracting data from an HTML page, you must view the HTML. To view HTML using Chrome, right-click the page and then click **View page source** (to view the entire page as a flat file) or click **Inspect** (to view an HTML inspection tool to navigate the page elements).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Beautiful Soup\n",
    "Beautiful Soup is a Python library for pulling data out of HTML and XML files. It works with your favorite parser to provide idiomatic ways of navigating, searching, and modifying the parse tree. It commonly saves programmers hours or days of work. (source: https://www.crummy.com/software/BeautifulSoup/bs4/doc/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Beautiful Soup\n",
    "To install the beautiful soup package (bs4) use pip.\n",
    "```\n",
    "pip install bs4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Requests module\n",
    "In addition to the HTML parser (bs4), you also need a method to fetch URLs (uniform resource locators). There are several options available to you. This example uses the requests package.\n",
    "\n",
    "```python\n",
    "import bs4\n",
    "from urllib.request import urlopen as req\n",
    "from bs4 import BeautifulSoup as soup\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 # parses HTML\n",
    "import requests\n",
    "#from urllib.request import urlopen as req # requests HTML\n",
    "from bs4 import BeautifulSoup as soup\n",
    "#import lxml as lh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Quotes\n",
    "Below is an example of how to use Requests and Beautiful Soup to obtain an HTML page and parse it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://wisdomquotes.com/quote-of-the-day/\"\n",
    "\n",
    "# Unless you send a header, a web server may reject your request\n",
    "headers = {'User-Agent' : 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'}\n",
    "\n",
    "#Request page, send headers\n",
    "page_html = requests.get(url, headers=headers)\n",
    "\n",
    "#DEBUG:print(page_html.content)\n",
    "\n",
    "# Use .text to return string (.content returns bytes)\n",
    "page_soup = soup(page_html.text, \"html.parser\")\n",
    "\n",
    "page_soup.title # Print the page title to confirm we have successfully parsed the web page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, inpsect the HTML to find a unique method to identify the information you wish to extract.\n",
    "\n",
    "All quotes are prefaced with \"blockquote\", which makes it easy to filter all the quotes on the page.\n",
    "\n",
    "Use the ```find_all()``` method to return a list of quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The world belongs to those who set out to conquer it armed with self confidence and good humour. Charles Dickens\n"
     ]
    }
   ],
   "source": [
    "quotes = page_soup.find_all(\"blockquote\")\n",
    "print(quotes[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through the list of quotes and print the text attribute of the element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for quote in quotes:\n",
    "    print(\"-----------------------------------\")\n",
    "    print(quote.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XPATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 <tr> elements found.\n",
      "\n",
      "1 W4 Milwaukee\n",
      "2 L1 Toronto\n",
      "3 W2 Indiana\n",
      "4 L1 Philadelphia\n",
      "5 L2 Boston\n",
      "1 L1 Golden St.\n",
      "2 W4 Denver\n",
      "3 L1 Oklahoma City\n",
      "4 W3 Portland\n",
      "5 W1 Houston\n"
     ]
    }
   ],
   "source": [
    "import lxml.html\n",
    "import requests\n",
    "\n",
    "html_response = requests.get(\"https://www.cbssports.com/nba/standings/\")\n",
    "\n",
    "# Results in an HtmlElement object which has the xpath method.\n",
    "doc = lxml.html.fromstring(html_response.content)\n",
    "\n",
    "# Use xpath syntax to filter html elements\n",
    "teams = doc.xpath('//tr')\n",
    "\n",
    "print(str(len(teams)) + \" <tr> elements found.\\n\")\n",
    "\n",
    "# list[col][row] - 1 = header\n",
    "for i in (2,3,4,5,6,19,20,21,22,23): # Dangerous assuption: row numbers cannot change\n",
    "    ranking = teams[i][0].text_content().strip()\n",
    "    team_name = teams[i][1].text_content().strip()\n",
    "    wl_streak = teams[i][13].text_content().strip()\n",
    "        \n",
    "    print(ranking, wl_streak, team_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
